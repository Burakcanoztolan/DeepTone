{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA9hE1CVtEMC",
        "outputId": "a55a4e82-2651-4ae8-8169-e591028c938f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Collecting resampy\n",
            "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from opendatasets) (8.3.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.4.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (5.29.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: resampy, opendatasets\n",
            "Successfully installed opendatasets-0.1.22 resampy-0.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets librosa gradio resampy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQepmFQZsekk",
        "outputId": "e484797f-d07e-44b7-b598-58c99f1bec02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ],
      "source": [
        "# model.py\n",
        "\n",
        "%%writefile model.py\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential # <--- Unutulan kısım burasıydı!\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, BatchNormalization\n",
        "\n",
        "def create_hybrid_model(input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "\n",
        "    # CNN Kısmı (Özellik Çıkarıcı)\n",
        "    # BatchNormalization ekledik, eğitimi hızlandırır\n",
        "    model.add(Conv1D(128, kernel_size=5, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    model.add(Conv1D(64, kernel_size=5, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    # LSTM Kısmı (Zaman Analizi)\n",
        "    # CNN çıkışını LSTM'e veriyoruz. Flatten yapmıyoruz!\n",
        "    # return_sequences=True diyoruz çünkü bir sonraki de LSTM katmanı\n",
        "    model.add(LSTM(64, return_sequences=True))\n",
        "    model.add(LSTM(32)) # Son karar için tek vektör\n",
        "\n",
        "    # Sınıflandırma Kısmı\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8JzAEQOsz9c",
        "outputId": "f529446f-56ad-4087-8c79-7492770cd73a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ],
      "source": [
        "# train.py (GÜNCELLENMİŞ - NORMALİZASYON EKLENDİ)\n",
        "\n",
        "%%writefile train.py\n",
        "\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import opendatasets as od\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from model import create_hybrid_model\n",
        "\n",
        "# --- 1. VERİ SETLERİNİ İNDİR ---\n",
        "# TESS İndir\n",
        "if not os.path.exists(\"./toronto-emotional-speech-set-tess\"):\n",
        "    od.download(\"https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess\")\n",
        "\n",
        "# RAVDESS İndir (YENİ!)\n",
        "if not os.path.exists(\"./ravdess-emotional-speech-audio\"):\n",
        "    od.download(\"https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio\")\n",
        "\n",
        "DATA_PATH_TESS = \"/content/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data\"\n",
        "DATA_PATH_RAVDESS = \"/content/ravdess-emotional-speech-audio\"\n",
        "\n",
        "MODEL_SAVE_PATH = \"duygu_modeli.h5\"\n",
        "LABELS_SAVE_PATH = \"etiketler.npy\"\n",
        "SCALER_SAVE_PATH = \"scaler.save\"\n",
        "\n",
        "# --- AUGMENTATION ---\n",
        "def noise(data):\n",
        "    noise_amp = 0.035 * np.random.uniform() * np.amax(data)\n",
        "    data = data + noise_amp * np.random.normal(size=data.shape[0])\n",
        "    return data\n",
        "\n",
        "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
        "    return librosa.effects.pitch_shift(y=data, sr=sampling_rate, n_steps=pitch_factor)\n",
        "\n",
        "# --- VERİ İŞLEME VE BİRLEŞTİRME ---\n",
        "data = []\n",
        "print(\"Veriler taranıyor (TESS + RAVDESS)...\")\n",
        "\n",
        "# 1. TESS VERİLERİNİ OKU\n",
        "print(\"TESS işleniyor...\")\n",
        "for dizin, alt_dizinler, dosyalar in os.walk(DATA_PATH_TESS):\n",
        "    for dosya in dosyalar:\n",
        "        if dosya.endswith(\".wav\"):\n",
        "            # TESS formatı: OAF_happy.wav -> 'happy'\n",
        "            duygu = dosya.split('_')[-1].split('.')[0].lower()\n",
        "            dosya_tam_yolu = os.path.join(dizin, dosya)\n",
        "            try:\n",
        "                audio, sr = librosa.load(dosya_tam_yolu, res_type='kaiser_fast')\n",
        "                # TESS verisini ekle (Augmentation ile)\n",
        "                data.append([np.mean(librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40).T, axis=0), duygu])\n",
        "                data.append([np.mean(librosa.feature.mfcc(y=noise(audio), sr=sr, n_mfcc=40).T, axis=0), duygu])\n",
        "            except: pass\n",
        "\n",
        "# 2. RAVDESS VERİLERİNİ OKU (YENİ!)\n",
        "print(\"RAVDESS işleniyor...\")\n",
        "# RAVDESS Duygu Haritası (Sayı -> Yazı)\n",
        "ravdess_map = {\n",
        "    '01': 'neutral', '02': 'neutral', # Calm'ı da Neutral yapıyoruz\n",
        "    '03': 'happy', '04': 'sad', '05': 'angry',\n",
        "    '06': 'fear', '07': 'disgust', '08': 'surprise'\n",
        "}\n",
        "\n",
        "for dizin, alt_dizinler, dosyalar in os.walk(DATA_PATH_RAVDESS):\n",
        "    for dosya in dosyalar:\n",
        "        if dosya.endswith(\".wav\"):\n",
        "            try:\n",
        "                # Dosya ismi örn: 03-01-06-01-01-01-01.wav (3. parça '06' yani fear)\n",
        "                parcalar = dosya.split('-')\n",
        "                if len(parcalar) > 2:\n",
        "                    duygu_kodu = parcalar[2]\n",
        "                    if duygu_kodu in ravdess_map:\n",
        "                        duygu = ravdess_map[duygu_kodu]\n",
        "\n",
        "                        # TESS ile etiket uyumu sağlamak için 'surprise' -> 'pleasant_surprise' düzeltmesi\n",
        "                        if duygu == 'surprise': duygu = 'pleasant_surprise'\n",
        "                        if duygu == 'fear': duygu = 'fear' # TESS'te klasör adı neyse o olmalı\n",
        "\n",
        "                        dosya_tam_yolu = os.path.join(dizin, dosya)\n",
        "                        audio, sr = librosa.load(dosya_tam_yolu, res_type='kaiser_fast')\n",
        "\n",
        "                        # RAVDESS verisini ekle (Sadece orjinal ve gürültülü)\n",
        "                        data.append([np.mean(librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40).T, axis=0), duygu])\n",
        "                        data.append([np.mean(librosa.feature.mfcc(y=noise(audio), sr=sr, n_mfcc=40).T, axis=0), duygu])\n",
        "            except: pass\n",
        "\n",
        "df = pd.DataFrame(data, columns=['Ozellikler', 'Duygu_Etiketi'])\n",
        "print(f\"Toplam Birleştirilmiş Veri Sayısı: {len(df)}\")\n",
        "\n",
        "# --- HAZIRLIK ---\n",
        "X = np.array(df['Ozellikler'].tolist())\n",
        "y = np.array(df['Duygu_Etiketi'].tolist())\n",
        "\n",
        "# Scaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "joblib.dump(scaler, SCALER_SAVE_PATH)\n",
        "\n",
        "# Etiketleme\n",
        "le = LabelEncoder()\n",
        "y_encoded = to_categorical(le.fit_transform(y))\n",
        "np.save(LABELS_SAVE_PATH, le.classes_)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# --- EĞİTİM ---\n",
        "print(\"Model iki veri setiyle eğitiliyor...\")\n",
        "model = create_hybrid_model(input_shape=(40, 1), num_classes=y_encoded.shape[1])\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(learning_rate=0.0005)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_cnn, y_train,\n",
        "    batch_size=64, # Veri çok olduğu için batch arttı\n",
        "    epochs=50,\n",
        "    validation_data=(X_test_cnn, y_test),\n",
        "    callbacks=[early_stop, checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Grafikler\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(history.history['accuracy'], label='Eğitim')\n",
        "plt.plot(history.history['val_accuracy'], label='Test')\n",
        "plt.title(f\"Model Başarısı (Veri Sayısı: {len(df)})\")\n",
        "plt.legend()\n",
        "plt.savefig(\"basari_grafigi.png\")\n",
        "plt.show()\n",
        "\n",
        "y_pred = np.argmax(model.predict(X_test_cnn), axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_, cmap='Blues')\n",
        "plt.savefig(\"confusion_matrix.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_lUQ5f-s7gK",
        "outputId": "4e04091c-8733-406c-ce8d-3c685c7fc4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing serve.py\n"
          ]
        }
      ],
      "source": [
        "# serve.py (SCALER EKLENDİ)\n",
        "\n",
        "%%writefile serve.py\n",
        "\n",
        "import gradio as gr\n",
        "import librosa\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import joblib # Scaler'ı yüklemek için\n",
        "\n",
        "MODEL_PATH = \"duygu_modeli.h5\"\n",
        "LABELS_PATH = \"etiketler.npy\"\n",
        "SCALER_PATH = \"scaler.save\" # Scaler dosyamız\n",
        "\n",
        "print(\"Sistem yükleniyor...\")\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "etiketler = np.load(LABELS_PATH, allow_pickle=True)\n",
        "scaler = joblib.load(SCALER_PATH) # Eğittiğimiz matematiği geri çağırıyoruz\n",
        "\n",
        "def tahmin_et(ses_dosyasi):\n",
        "    if ses_dosyasi is None:\n",
        "        return \"Lütfen ses dosyası yükleyin.\"\n",
        "\n",
        "    # Sesi işle\n",
        "    audio, sample_rate = librosa.load(ses_dosyasi, res_type='kaiser_fast')\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "    mfccs_scaled_raw = np.mean(mfccs.T, axis=0)\n",
        "\n",
        "    # --- KRİTİK ADIM: NORMALİZASYON ---\n",
        "    # Gelen sesi de aynı şekilde sıkıştırıyoruz\n",
        "    # (reshape(1, -1) tek bir örnek olduğu için gerekli)\n",
        "    mfccs_std = scaler.transform(mfccs_scaled_raw.reshape(1, -1))\n",
        "\n",
        "    # Modele uygun boyuta getir\n",
        "    veri = mfccs_std.reshape(1, 40, 1)\n",
        "\n",
        "    # Tahmin\n",
        "    olasiliklar = model.predict(veri)\n",
        "    tahmin_index = np.argmax(olasiliklar, axis=1)[0]\n",
        "    sonuc = etiketler[tahmin_index]\n",
        "\n",
        "    return f\"Sonuç: {sonuc.upper()}\"\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=tahmin_et,\n",
        "    inputs=gr.Audio(type=\"filepath\", label=\"Test Et\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Hibrit DeepTone Analizi\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2YZhl2SgtZs",
        "outputId": "3f36e0bb-eb08-46f2-f3c7-593c97f55f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-26 09:48:10.427926: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766742490.473761     302 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766742490.494437     302 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766742490.561922     302 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766742490.561956     302 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766742490.561961     302 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766742490.561965     302 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-26 09:48:10.570660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: Burakcanztolan\n",
            "Your Kaggle Key: \n",
            "Dataset URL: https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess\n",
            "Downloading toronto-emotional-speech-set-tess.zip to ./toronto-emotional-speech-set-tess\n",
            " 90% 383M/428M [00:02<00:00, 149MB/s]\n",
            "100% 428M/428M [00:02<00:00, 166MB/s]\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: Burakcanztolan\n",
            "Your Kaggle Key: \n",
            "Dataset URL: https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio\n",
            "Downloading ravdess-emotional-speech-audio.zip to ./ravdess-emotional-speech-audio\n",
            " 87% 372M/429M [00:02<00:00, 105MB/s]\n",
            "100% 429M/429M [00:02<00:00, 164MB/s]\n",
            "Veriler taranıyor (TESS + RAVDESS)...\n",
            "TESS işleniyor...\n",
            "RAVDESS işleniyor...\n",
            "Toplam Birleştirilmiş Veri Sayısı: 11360\n",
            "Model iki veri setiyle eğitiliyor...\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "2025-12-26 09:56:20.952781: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1766742980.954368     302 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "Epoch 1/50\n",
            "I0000 00:00:1766742985.235949    2380 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
            "\u001b[1m136/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2872 - loss: 1.8936\n",
            "Epoch 1: val_accuracy improved from -inf to 0.51452, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - accuracy: 0.2924 - loss: 1.8834 - val_accuracy: 0.5145 - val_loss: 1.6071\n",
            "Epoch 2/50\n",
            "\u001b[1m140/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5815 - loss: 1.2038\n",
            "Epoch 2: val_accuracy improved from 0.51452 to 0.63864, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5821 - loss: 1.2022 - val_accuracy: 0.6386 - val_loss: 1.1259\n",
            "Epoch 3/50\n",
            "\u001b[1m136/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6550 - loss: 0.9586\n",
            "Epoch 3: val_accuracy improved from 0.63864 to 0.67694, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6556 - loss: 0.9570 - val_accuracy: 0.6769 - val_loss: 0.9007\n",
            "Epoch 4/50\n",
            "\u001b[1m138/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7043 - loss: 0.8309\n",
            "Epoch 4: val_accuracy improved from 0.67694 to 0.73019, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7046 - loss: 0.8303 - val_accuracy: 0.7302 - val_loss: 0.7590\n",
            "Epoch 5/50\n",
            "\u001b[1m140/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7299 - loss: 0.7635\n",
            "Epoch 5: val_accuracy improved from 0.73019 to 0.75704, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7299 - loss: 0.7634 - val_accuracy: 0.7570 - val_loss: 0.6579\n",
            "Epoch 6/50\n",
            "\u001b[1m138/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7469 - loss: 0.6905\n",
            "Epoch 6: val_accuracy improved from 0.75704 to 0.77641, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7471 - loss: 0.6904 - val_accuracy: 0.7764 - val_loss: 0.6274\n",
            "Epoch 7/50\n",
            "\u001b[1m140/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7858 - loss: 0.6170\n",
            "Epoch 7: val_accuracy improved from 0.77641 to 0.78169, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7857 - loss: 0.6171 - val_accuracy: 0.7817 - val_loss: 0.6033\n",
            "Epoch 8/50\n",
            "\u001b[1m140/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7949 - loss: 0.5869\n",
            "Epoch 8: val_accuracy did not improve from 0.78169\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7949 - loss: 0.5868 - val_accuracy: 0.7790 - val_loss: 0.5929\n",
            "Epoch 9/50\n",
            "\u001b[1m138/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8143 - loss: 0.5229\n",
            "Epoch 9: val_accuracy improved from 0.78169 to 0.80766, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8143 - loss: 0.5230 - val_accuracy: 0.8077 - val_loss: 0.5283\n",
            "Epoch 10/50\n",
            "\u001b[1m140/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8312 - loss: 0.4767\n",
            "Epoch 10: val_accuracy improved from 0.80766 to 0.82526, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8312 - loss: 0.4767 - val_accuracy: 0.8253 - val_loss: 0.4870\n",
            "Epoch 11/50\n",
            "\u001b[1m139/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8530 - loss: 0.4316\n",
            "Epoch 11: val_accuracy improved from 0.82526 to 0.82879, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8529 - loss: 0.4315 - val_accuracy: 0.8288 - val_loss: 0.4926\n",
            "Epoch 12/50\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8596 - loss: 0.4106\n",
            "Epoch 12: val_accuracy did not improve from 0.82879\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8596 - loss: 0.4106 - val_accuracy: 0.8059 - val_loss: 0.5534\n",
            "Epoch 13/50\n",
            "\u001b[1m139/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8714 - loss: 0.3733\n",
            "Epoch 13: val_accuracy improved from 0.82879 to 0.83583, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8714 - loss: 0.3731 - val_accuracy: 0.8358 - val_loss: 0.4651\n",
            "Epoch 14/50\n",
            "\u001b[1m139/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8826 - loss: 0.3464\n",
            "Epoch 14: val_accuracy improved from 0.83583 to 0.86092, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8824 - loss: 0.3467 - val_accuracy: 0.8609 - val_loss: 0.4094\n",
            "Epoch 15/50\n",
            "\u001b[1m139/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8983 - loss: 0.2995\n",
            "Epoch 15: val_accuracy did not improve from 0.86092\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8980 - loss: 0.3002 - val_accuracy: 0.8486 - val_loss: 0.4185\n",
            "Epoch 16/50\n",
            "\u001b[1m138/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9003 - loss: 0.2880\n",
            "Epoch 16: val_accuracy improved from 0.86092 to 0.87192, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9003 - loss: 0.2881 - val_accuracy: 0.8719 - val_loss: 0.4077\n",
            "Epoch 17/50\n",
            "\u001b[1m137/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9094 - loss: 0.2711\n",
            "Epoch 17: val_accuracy did not improve from 0.87192\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9092 - loss: 0.2715 - val_accuracy: 0.8697 - val_loss: 0.3668\n",
            "Epoch 18/50\n",
            "\u001b[1m139/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9218 - loss: 0.2454\n",
            "Epoch 18: val_accuracy did not improve from 0.87192\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9216 - loss: 0.2459 - val_accuracy: 0.8693 - val_loss: 0.3758\n",
            "Epoch 19/50\n",
            "\u001b[1m140/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9245 - loss: 0.2168\n",
            "Epoch 19: val_accuracy improved from 0.87192 to 0.87456, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9244 - loss: 0.2172 - val_accuracy: 0.8746 - val_loss: 0.3931\n",
            "Epoch 20/50\n",
            "\u001b[1m140/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9284 - loss: 0.2201\n",
            "Epoch 20: val_accuracy did not improve from 0.87456\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9283 - loss: 0.2202 - val_accuracy: 0.8680 - val_loss: 0.4076\n",
            "Epoch 21/50\n",
            "\u001b[1m141/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9363 - loss: 0.1910\n",
            "Epoch 21: val_accuracy improved from 0.87456 to 0.87500, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9362 - loss: 0.1913 - val_accuracy: 0.8750 - val_loss: 0.3863\n",
            "Epoch 22/50\n",
            "\u001b[1m136/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9375 - loss: 0.1946\n",
            "Epoch 22: val_accuracy improved from 0.87500 to 0.87764, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9374 - loss: 0.1944 - val_accuracy: 0.8776 - val_loss: 0.3729\n",
            "Epoch 23/50\n",
            "\u001b[1m139/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9475 - loss: 0.1517\n",
            "Epoch 23: val_accuracy did not improve from 0.87764\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9473 - loss: 0.1522 - val_accuracy: 0.8587 - val_loss: 0.4351\n",
            "Epoch 24/50\n",
            "\u001b[1m141/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9427 - loss: 0.1715\n",
            "Epoch 24: val_accuracy improved from 0.87764 to 0.88996, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9427 - loss: 0.1714 - val_accuracy: 0.8900 - val_loss: 0.3542\n",
            "Epoch 25/50\n",
            "\u001b[1m138/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9488 - loss: 0.1571\n",
            "Epoch 25: val_accuracy did not improve from 0.88996\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9488 - loss: 0.1570 - val_accuracy: 0.8891 - val_loss: 0.3855\n",
            "Epoch 26/50\n",
            "\u001b[1m138/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9565 - loss: 0.1320\n",
            "Epoch 26: val_accuracy did not improve from 0.88996\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9565 - loss: 0.1319 - val_accuracy: 0.8847 - val_loss: 0.3834\n",
            "Epoch 27/50\n",
            "\u001b[1m138/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9613 - loss: 0.1270\n",
            "Epoch 27: val_accuracy improved from 0.88996 to 0.89173, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9611 - loss: 0.1274 - val_accuracy: 0.8917 - val_loss: 0.3540\n",
            "Epoch 28/50\n",
            "\u001b[1m140/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9578 - loss: 0.1340\n",
            "Epoch 28: val_accuracy did not improve from 0.89173\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9577 - loss: 0.1342 - val_accuracy: 0.8754 - val_loss: 0.4497\n",
            "Epoch 29/50\n",
            "\u001b[1m139/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9555 - loss: 0.1383\n",
            "Epoch 29: val_accuracy improved from 0.89173 to 0.91285, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9556 - loss: 0.1381 - val_accuracy: 0.9129 - val_loss: 0.3294\n",
            "Epoch 30/50\n",
            "\u001b[1m137/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9655 - loss: 0.1017\n",
            "Epoch 30: val_accuracy did not improve from 0.91285\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9654 - loss: 0.1023 - val_accuracy: 0.9062 - val_loss: 0.3372\n",
            "Epoch 31/50\n",
            "\u001b[1m140/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9692 - loss: 0.0957\n",
            "Epoch 31: val_accuracy did not improve from 0.91285\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9691 - loss: 0.0959 - val_accuracy: 0.9014 - val_loss: 0.3540\n",
            "Epoch 32/50\n",
            "\u001b[1m137/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9690 - loss: 0.1019\n",
            "Epoch 32: val_accuracy did not improve from 0.91285\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9687 - loss: 0.1024 - val_accuracy: 0.8776 - val_loss: 0.4174\n",
            "Epoch 33/50\n",
            "\u001b[1m139/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9655 - loss: 0.1121\n",
            "Epoch 33: val_accuracy did not improve from 0.91285\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9655 - loss: 0.1120 - val_accuracy: 0.9032 - val_loss: 0.4035\n",
            "Epoch 34/50\n",
            "\u001b[1m136/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9675 - loss: 0.1047\n",
            "Epoch 34: val_accuracy did not improve from 0.91285\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9673 - loss: 0.1051 - val_accuracy: 0.8988 - val_loss: 0.3489\n",
            "Epoch 35/50\n",
            "\u001b[1m139/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9748 - loss: 0.0811\n",
            "Epoch 35: val_accuracy did not improve from 0.91285\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9748 - loss: 0.0811 - val_accuracy: 0.8996 - val_loss: 0.3648\n",
            "Epoch 36/50\n",
            "\u001b[1m139/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9725 - loss: 0.0868\n",
            "Epoch 36: val_accuracy did not improve from 0.91285\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9724 - loss: 0.0868 - val_accuracy: 0.8904 - val_loss: 0.4167\n",
            "Epoch 37/50\n",
            "\u001b[1m137/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9632 - loss: 0.1106\n",
            "Epoch 37: val_accuracy did not improve from 0.91285\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9633 - loss: 0.1105 - val_accuracy: 0.9129 - val_loss: 0.3249\n",
            "Epoch 38/50\n",
            "\u001b[1m138/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9740 - loss: 0.0768\n",
            "Epoch 38: val_accuracy improved from 0.91285 to 0.91373, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9740 - loss: 0.0770 - val_accuracy: 0.9137 - val_loss: 0.3626\n",
            "Epoch 39/50\n",
            "\u001b[1m139/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9824 - loss: 0.0576\n",
            "Epoch 39: val_accuracy improved from 0.91373 to 0.91725, saving model to duygu_modeli.h5\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9824 - loss: 0.0579 - val_accuracy: 0.9173 - val_loss: 0.3386\n",
            "Epoch 40/50\n",
            "\u001b[1m140/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9804 - loss: 0.0639\n",
            "Epoch 40: val_accuracy did not improve from 0.91725\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9803 - loss: 0.0641 - val_accuracy: 0.9120 - val_loss: 0.3584\n",
            "Epoch 41/50\n",
            "\u001b[1m139/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9751 - loss: 0.0752\n",
            "Epoch 41: val_accuracy did not improve from 0.91725\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9750 - loss: 0.0755 - val_accuracy: 0.9027 - val_loss: 0.3909\n",
            "Epoch 42/50\n",
            "\u001b[1m138/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9751 - loss: 0.0779\n",
            "Epoch 42: val_accuracy did not improve from 0.91725\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9750 - loss: 0.0782 - val_accuracy: 0.8957 - val_loss: 0.3927\n",
            "Epoch 43/50\n",
            "\u001b[1m140/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9742 - loss: 0.0793\n",
            "Epoch 43: val_accuracy did not improve from 0.91725\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9742 - loss: 0.0793 - val_accuracy: 0.9049 - val_loss: 0.3953\n",
            "Epoch 44/50\n",
            "\u001b[1m139/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9722 - loss: 0.0827\n",
            "Epoch 44: val_accuracy did not improve from 0.91725\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9723 - loss: 0.0826 - val_accuracy: 0.9093 - val_loss: 0.3676\n",
            "Epoch 45/50\n",
            "\u001b[1m139/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9764 - loss: 0.0745\n",
            "Epoch 45: val_accuracy did not improve from 0.91725\n",
            "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9763 - loss: 0.0747 - val_accuracy: 0.9115 - val_loss: 0.3562\n",
            "Figure(1000x400)\n",
            "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Figure(1000x800)\n"
          ]
        }
      ],
      "source": [
        "!python train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVGeW10dguvx",
        "outputId": "8929be06-ed6d-4285-b415-bcd8eed76062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-26 09:57:52.997963: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766743073.042570    3527 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766743073.062024    3527 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766743073.086165    3527 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766743073.086204    3527 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766743073.086213    3527 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766743073.086221    3527 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-26 09:57:53.093104: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Sistem yükleniyor...\n",
            "2025-12-26 09:58:03.392320: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1766743083.392467    3527 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://53dd253935a6fe01f4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "I0000 00:00:1766743149.961255    3657 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 700ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3043, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/serve.py\", line 49, in <module>\n",
            "    interface.launch(share=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2950, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3045, in block_thread\n",
            "    print(\"Keyboard interruption in main thread... closing server.\")\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://53dd253935a6fe01f4.gradio.live\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python serve.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}